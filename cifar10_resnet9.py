# -*- coding: utf-8 -*-
"""CIFAR10_ResNet9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WQ6RD1S5dKYPz0dQwTzjxyNiKTgKLjIY
"""

import os
import torch
import torchvision
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader, random_split
import torchvision.transforms as tt
from torch.utils.data import random_split
from torchvision.utils import make_grid
import matplotlib
import matplotlib.pyplot as plt

stats = [(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)]
train_tfm = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'),
                        tt.RandomHorizontalFlip(),
                        tt.ToTensor(),
                        tt.Normalize(*stats, inplace=True)])
val_tfm = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])
val_ds = CIFAR10(
    root='./data', train=False, download=True, transform=val_tfm)
train_ds = CIFAR10(
    root='./data', train=True, download=True, transform=train_tfm)

batch_size = 128
train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)
val_dl = DataLoader(val_ds, batch_size, num_workers=2, pin_memory=True)

def get_default_device():
  if torch.cuda.is_available():
    return torch.device('cuda')
  else:
    return torch.device('cpu')

  
def to_device(data, device):
  if isinstance(data, (list, tuple)):
    return [to_device(x, device) for x in data]
  else:
    return data.to(device, non_blocking=True)


class DeviceDataLoader():
  def __init__(self, dl, device):
    self.dl = dl
    self.device = device
  
  def __iter__(self):
    for b in self.dl:
      yield to_device(b, self.device)
  
  def __len__(self):
    return len(self.dl)

def accuracy(output, label):
  _, pred = torch.max(output, dim=1)
  return torch.tensor(torch.sum(pred == label).item() / len(pred))

class CIFAR10ClassificationBase(nn.Module):

  def training_step(self, batch):
    image, label = batch
    out = self(image)
    loss = F.cross_entropy(out, label)
    acc = accuracy(out, label)
    return loss

  def validation_step(self, batch):
    image, label = batch
    out = self(image)
    loss = F.cross_entropy(out, label)
    acc = accuracy(out, label)
    return {'val_loss': loss.detach(), 'val_acc': acc}
  
  def validation_epoch_end(self, output):
    batch_losses = [x['val_loss'] for x in output]
    epoch_loss = torch.stack(batch_losses).mean()
    batch_accs = [x['val_acc'] for x in output]
    epoch_acc = torch.stack(batch_accs).mean()
    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}
  
  def epoch_end(self, epoch, result):
    print("Epoch [{}], last_lr: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}".format(
            epoch, result['lr'][-1], result['train_loss'], result['val_loss'], result['val_acc']))

def conv_block(in_channels, out_channels, pool=False):
  layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)]
  if pool: 
    layers.append(nn.MaxPool2d(2))
  return nn.Sequential(*layers)

class ResNet9(CIFAR10ClassificationBase):
  def __init__(self, in_channels, out_classes):
    super().__init__()

    self.conv1 = conv_block(in_channels, 64)
    self.conv2 = conv_block(64, 128, pool=True)
    self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))

    self.conv3 = conv_block(128, 256, pool=True)
    self.conv4 = conv_block(256, 512, pool=True)
    self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))

    self.classifier = nn.Sequential(nn.MaxPool2d(4),
                                    nn.Flatten(),
                                    nn.Dropout(0.2),
                                    nn.Linear(512, out_classes))
    
  def forward(self, xb):
    out = self.conv1(xb)
    out = self.conv2(out)
    out = self.res1(out) + out
    out = self.conv3(out)
    out = self.conv4(out)
    out = self.res2(out) + out
    out = self.classifier(out)
    return out

@torch.no_grad()
def evaluate(model, val_loader):
  model.eval()
  outputs = [model.validation_step(batch) for batch in val_loader]
  return model.validation_epoch_end(outputs)


def get_lr(optimizer):
  for param_group in optimizer.param_groups:
    return param_group['lr']


def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader
                  , weight_decay, grad_clip=0, opt_func=torch.optim.SGD):
  torch.cuda.empty_cache()
  history = []
  

  optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)
  scheduled = torch.optim.lr_scheduler.OneCycleLR(
      optimizer, max_lr, epochs=epochs, steps_per_epoch = len(train_loader))
  

  for epoch in range(epochs):
    model.train()
    train_losses = []
    lrs = []
    for batch in train_loader:
      loss = model.training_step(batch)
      train_losses.append(loss)
      loss.backward()
    
      if grad_clip:
        nn.utils.clip_grad_value_(model.parameters(), grad_clip)
      
      optimizer.step()
      optimizer.zero_grad()

      lrs.append(get_lr(optimizer))
      scheduled.step()

    result = evaluate(model, val_loader)
    result['train_loss'] = torch.stack(train_losses).mean().item()
    result['lr'] = lrs
    model.epoch_end(epoch, result)
    history.append(result)
  return history

device = get_default_device()
model = to_device(ResNet9(3, 10), device)
train_dl = DeviceDataLoader(train_dl, device)
val_dl = DeviceDataLoader(val_dl, device)

epochs = 10
max_lr = 0.01
grad_clip = 0.1
weight_decay = 0.0001
opt_func = torch.optim.Adam


history = [evaluate(model, val_dl)]
history += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl
                         , grad_clip=grad_clip, weight_decay=weight_decay, 
                         opt_func=opt_func)

torch.save(model.state_dict(), 'CIFAR10_ResNet9.pth')

def loss_vs_epoch(history):
  train_ls = [result.get('train_loss') for result in history]
  val_ls = [result['val_loss'] for result in history]
  plt.plot(train_ls, '-ro')
  plt.plot(val_ls, '-bo')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend(['Train', 'Validation'])
  plt.title('Loss vs Epoch')

def acc_vs_epoch(history):
  val_ac = [result['val_acc'] for result in history]
  plt.plot(val_ac, '-x')
  plt.xlabel('Accuracy')
  plt.ylabel('Epoch')
  plt.title('Accuracy vs Epoch')

def plot_lr(history):
  learning_rate = np.concatenate([result.get('lr', []) for result in history])
  plt.plot(learning_rate)
  plt.ylabel('Learning rate')
  plt.xlabel('Batch')
  plt.title('Learning Rate Curve')

loss_vs_epoch(history)

acc_vs_epoch(history)

plot_lr(history)